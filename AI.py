import math
import random
import numpy as np
import pandas as pd
import tensorflow as tf
from collections import deque
import matplotlib.pyplot as plt
from keras.layers import Dense, Dropout
from keras.models import Sequential
from keras.optimizers import Adam, RMSprop
from sklearn.metrics import accuracy_score


def set_seeds(seed=100): 
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

class observation_space: 
    def __init__(self, n):
        self.shape = (n,)
    
class action_space:
    def __init__(self, n):
        self.n = n
    def seed(self, seed):
        pass
    def sample(self):
        return random.randint(0, self.n - 1)
        
class Finance:
    
    url = 'http://hilpisch.com/aiif_eikon_eod_data.csv'
    
    def __init__(self, symbol, features, window, lags, leverage=1, min_performance=0.85, start=0, end=None, mu=None, std=None):
        self.symbol = symbol
        self.features = features
        self.n_features = len(features)
        self.window = window
        self.lags = lags
        self.leverage = leverage
        self.min_performance = min_performance
        self.start = start
        self.end = end
        self.mu = mu
        self.std = std
        self.observation_space = observation_space(self.lags)
        self.action_space = action_space(2)
        self._get_data()
        self._prepare_data()
        
    def _get_data(self):
        self.raw = pd.read_csv(self.url, index_col=0, parse_dates=True).dropna()

    def _prepare_data(self):
        self.data = pd.DataFrame(self.raw[self.symbol]) 
        self.data = self.data.iloc[self.start:]
        self.data['r'] = np.log(self.data / self.data.shift(1)) 
        self.data.dropna(inplace=True)
        self.data['s'] = self.data[self.symbol].rolling(self.window).mean()
        self.data['m'] = self.data['r'].rolling(self.window).mean()
        self.data['v'] = self.data['r'].rolling(self.window).std() 
        self.data.dropna(inplace=True)
        if self.mu is None:
            self.mu = self.data.mean()
            self.std = self.data.std()
        self.data_ = (self.data - self.mu) / self.std 
        self.data_['d'] = np.where(self.data['r'] > 0, 1, 0) 
        self.data_['d'] = self.data_['d'].astype(int)
        if self.end is not None:
            self.data = self.data.iloc[:self.end - self.start]
            self.data_ = self.data_.iloc[:self.end - self.start]

    def _get_state(self):
        return self.data_[self.features].iloc[self.bar - self.lags:self.bar]

    def seed(self, seed): 
        random.seed(seed) 
        np.random.seed(seed)

    def reset(self): 
        self.treward = 0
        self.accuracy = 0
        self.performance = 1
        self.bar = self.lags
        state = self.data_[self.features].iloc[self.bar-self.lags:self.bar]
        return state.values
    
    def step(self, action):
        correct = action == self.data_['d'].iloc[self.bar] 
        ret = self.data['r'].iloc[self.bar] * self.leverage 
        reward_1 = 1 if correct else 0
        reward_2 = abs(ret) if correct else -abs(ret)
        factor = 1 if correct else -1
        self.treward += reward_1
        self.bar += 1
        self.accuracy = self.treward / (self.bar - self.lags) 
        self.performance *= math.exp(reward_2)
        if self.bar >= len(self.data):
            done = True 
        elif reward_1 == 1:
            done = False
        elif (self.performance < self.min_performance and self.bar > self.lags + 5):
            done = True
        else:
            done = False
        state = self._get_state()
        info = {}
        return state.values, reward_1 + reward_2 * 5, done, info
      
      
class FQLAgent:
    
    def __init__(self, hidden_units, learning_rate, learn_env, valid_env):
        self.learn_env = learn_env
        self.valid_env = valid_env
        self.epsilon = 1.0
        self.epsilon_min = 0.1
        self.epsilon_decay = 0.98
        self.learning_rate = learning_rate
        self.gamma = 0.95
        self.batch_size = 128
        self.max_treward = 0
        self.trewards = list()
        self.averages = list()
        self.performances = list()
        self.aperformances = list()
        self.vperformances = list()
        self.memory = deque(maxlen=2000)
        self.model = self._build_model(hidden_units, learning_rate)
        
    def _build_model(self, hu, lr): 
        model = Sequential() 
        model.add(Dense(hu, input_shape=(self.learn_env.lags, self.learn_env.n_features), activation='relu'))
        model.add(Dropout(0.3, seed=100))
        model.add(Dense(hu, activation='relu'))
        model.add(Dropout(0.3, seed=100))
        model.add(Dense(2, activation='linear'))
        model.compile(loss='mse', optimizer=RMSprop(lr=lr))
        return model

    def act(self, state):
        if random.random() <= self.epsilon:
            return self.learn_env.action_space.sample() 
        action = self.model.predict(state)[0, 0] 
        return np.argmax(action)

    def replay(self):
        batch = random.sample(self.memory, self.batch_size) 
        for state, action, reward, next_state, done in batch:
            if not done:
                reward += self.gamma * np.amax(self.model.predict(next_state)[0, 0])
            target = self.model.predict(state)
            target[0, 0, action] = reward
            self.model.fit(state, target, epochs=1, verbose=False) 
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def learn(self, episodes):
        for e in range(1, episodes + 1):
            state = self.learn_env.reset()
            state = np.reshape(state, [1, self.learn_env.lags, self.learn_env.n_features])
            for _ in range(10000):
                action = self.act(state) 
                next_state, reward, done, info = self.learn_env.step(action)
                next_state = np.reshape(next_state, [1, self.learn_env.lags, self.learn_env.n_features])
                self.memory.append([state, action, reward, next_state, done])
                state = next_state 
                if done:
                    treward=_+1 
                    self.trewards.append(treward)
                    av = sum(self.trewards[-25:]) / 25 
                    perf = self.learn_env.performance 
                    self.averages.append(av) 
                    self.performances.append(perf) 
                    self.aperformances.append(sum(self.performances[-25:]) / 25) 
                    self.max_treward = max(self.max_treward, treward) 
                    templ = 'episode: {:2d}/{} | treward: {:4d} | ' 
                    templ += 'perf: {:5.3f} | av: {:5.1f} | max: {:4d}' 
                    print(templ.format(e, episodes, treward, perf, av, self.max_treward), end='\r')
                    break
            self.validate(e, episodes)
            if len(self.memory) > self.batch_size:
                self.replay()
            
    def validate(self, e, episodes):
        state = self.valid_env.reset()
        state = np.reshape(state, [1, self.valid_env.lags, self.valid_env.n_features])
        for _ in range(10000):
            action = np.argmax(self.model.predict(state)[0, 0]) 
            next_state, reward, done, info = self.valid_env.step(action) 
            state = np.reshape(next_state, [1, self.valid_env.lags, self.valid_env.n_features])
            if done: 
                treward= _ + 1
                perf = self.valid_env.performance 
                self.vperformances.append(perf)
                if e % 20 == 0:
                    templ = 71 * '='
                    templ += '\nepisode: {:2d}/{} | VALIDATION | ' 
                    templ += 'treward: {:4d} | perf: {:5.3f} | ' 
                    templ += 'eps: {:.2f}\n'
                    templ += 71 * '='
                    print(templ.format(e, episodes, treward, perf, self.epsilon))
                break
                
if '__name__' == '__main__':
  
  symbol = 'EUR='
  features = [symbol, 'r', 's', 'm', 'v']

  a = 0
  b = 2000
  c = 500

  learn_env = Finance(symbol, features, window=10, lags=6, leverage=1,
                     min_performance=0.85, start=a, end=a+b, mu=None, std=None)

  valid_env = Finance(symbol, features, window=learn_env.window,
                     lags=learn_env.lags, leverage=learn_env.leverage, 
                      min_performance=learn_env.min_performance,
                     start=a+b, end=a+b+c, mu=learn_env.mu, std=learn_env.std)

  set_seeds(100)

  agent = FQLAgent(24, 0.0001, learn_env, valid_env)

  episodes = 61

  agent.learn(episodes)

  agent.epsilon

  plt.figure(figsize=(10, 6))
  x = range(1, len(agent.performances) + 1)
  y = np.polyval(np.polyfit(x, agent.performances, deg=3), x)
  y_ = np.polyval(np.polyfit(x, agent.vperformances, deg=3), x)
  plt.plot(agent.performances[:], label='training')
  plt.plot(agent.vperformances[:], label='validation')
  plt.plot(x, y, 'r--', label='regression (train)')
  plt.plot(x, y_, 'r-.', label='regression (valid)')
  plt.xlabel('episodes')
  plt.ylabel('gross performance')
  plt.legend();
